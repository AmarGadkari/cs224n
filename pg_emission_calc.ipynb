{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQKBtE7_AUwP",
        "outputId": "1c313e15-72cc-4c38-eb58-af42996320ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.5.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.27.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Accelerate version: 0.27.2\n",
            "Transformers version: 4.38.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas==1.5.3\n",
        "!pip install sentence-transformers accelerate -U\n",
        "!pip install transformers[torch] -U\n",
        "\n",
        "import accelerate\n",
        "import transformers\n",
        "print(\"Accelerate version:\", accelerate.__version__)\n",
        "print(\"Transformers version:\", transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive2.auth import GoogleAuth\n",
        "from pydrive2.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "kisSICzTaibq"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "epa_data = drive.CreateFile({'id': '1nOdK7YrAzx-2ZpKZCd620rbSG9qLLxZp'})\n",
        "epa_data.GetContentFile('EPA_EmissionsData.csv')\n",
        "factor_df = pd.read_csv('EPA_EmissionsData.csv')\n",
        "print(factor_df.head())\n",
        "\n",
        "activity_data = drive.CreateFile({'id': '1smUkjfTvvHmx2QiCfDImKOOtdCSb4dTk'})\n",
        "activity_data.GetContentFile('business_activities_training_data.csv')\n",
        "activity_df = pd.read_csv('business_activities_training_data.csv')\n",
        "print(activity_df.head())\n",
        "\n",
        "# Load the test data\n",
        "activity_test_data = drive.CreateFile({'id': '1Q6Sm-lxT-cpOJpFeLg_Leyj3w28Ff9GY'})\n",
        "activity_test_data.GetContentFile('business_activities_test_data.csv')\n",
        "test_df = pd.read_csv('business_activities_test_data.csv')\n",
        "print(test_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gK8yCqGkakrk",
        "outputId": "d93a8ee6-eae4-4b05-8a90-1c1940350efc"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   2017 NAICS Code                  2017 NAICS Title       GHG  \\\n",
            "0           111110                   Soybean Farming  All GHGs   \n",
            "1           111120  Oilseed (except Soybean) Farming  All GHGs   \n",
            "2           111130          Dry Pea and Bean Farming  All GHGs   \n",
            "3           111140                     Wheat Farming  All GHGs   \n",
            "4           111150                      Corn Farming  All GHGs   \n",
            "\n",
            "                                Unit  \\\n",
            "0  kg CO2e/2021 USD, purchaser price   \n",
            "1  kg CO2e/2021 USD, purchaser price   \n",
            "2  kg CO2e/2021 USD, purchaser price   \n",
            "3  kg CO2e/2021 USD, purchaser price   \n",
            "4  kg CO2e/2021 USD, purchaser price   \n",
            "\n",
            "   Supply Chain Emission Factors without Margins  \\\n",
            "0                                          1.223   \n",
            "1                                          1.223   \n",
            "2                                          2.874   \n",
            "3                                          2.874   \n",
            "4                                          2.874   \n",
            "\n",
            "   Margins of Supply Chain Emission Factors  \\\n",
            "0                                     0.103   \n",
            "1                                     0.103   \n",
            "2                                     0.134   \n",
            "3                                     0.134   \n",
            "4                                     0.134   \n",
            "\n",
            "   Supply Chain Emission Factors with Margins Reference USEEIO Code  \n",
            "0                                       1.326                1111A0  \n",
            "1                                       1.326                1111A0  \n",
            "2                                       3.007                1111B0  \n",
            "3                                       3.007                1111B0  \n",
            "4                                       3.007                1111B0  \n",
            "                Business Activity Description                Vendor  \\\n",
            "0          Forest management and conservation  EcoForest Management   \n",
            "1              Mineral processing consultancy    OreRefine Partners   \n",
            "2                 Cosmetic dentistry services   SmileDesign Experts   \n",
            "3          Forest management and conservation  EcoForest Management   \n",
            "4  Environmental impact assessment for mining    SustainAssess Ltd.   \n",
            "\n",
            "     Cost_USD                                            Comment  \\\n",
            "0  254.702489     Monitoring health and growth of forest tracts.   \n",
            "1  292.480465                     Maximizing ore recovery rates.   \n",
            "2  780.134077  Personalized cosmetic plans to enhance patient...   \n",
            "3  781.766215      Developing long-term forest management plans.   \n",
            "4  752.202177         Biodiversity preservation plans developed.   \n",
            "\n",
            "                      2017 NAICS Title  \n",
            "0              Timber Tract Operations  \n",
            "1  Support Activities for Metal Mining  \n",
            "2                  Offices of Dentists  \n",
            "3              Timber Tract Operations  \n",
            "4  Support Activities for Metal Mining  \n",
            "                Business Activity Description                 Vendor  \\\n",
            "0      Drilling services for zinc exploration         ZincDrill Corp   \n",
            "1      Drilling services for zinc exploration        ExploreZinc Co.   \n",
            "2      Drilling services for zinc exploration        ExploreZinc Co.   \n",
            "3  Environmental impact assessment for mining  GreenMine Consultants   \n",
            "4  Environmental impact assessment for mining     SustainAssess Ltd.   \n",
            "\n",
            "     Cost_USD                                            Comment  \\\n",
            "0  560.403441              Advanced geological analysis ensured.   \n",
            "1  193.140127              Advanced geological analysis ensured.   \n",
            "2  715.011342  Minimized environmental footprint during explo...   \n",
            "3  555.734481         Biodiversity preservation plans developed.   \n",
            "4  188.905865      Comprehensive ecosystem evaluation conducted.   \n",
            "\n",
            "                      2017 NAICS Title  \n",
            "0  Support Activities for Metal Mining  \n",
            "1  Support Activities for Metal Mining  \n",
            "2  Support Activities for Metal Mining  \n",
            "3  Support Activities for Metal Mining  \n",
            "4  Support Activities for Metal Mining  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n\\nFactor Count: {factor_df.count()}\")\n",
        "print(f\"\\n\\nActivity Count: {activity_df.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIeXWJs9db0N",
        "outputId": "b93b9b7a-6391-422f-e654-9360a7e0c515"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Factor Count: 2017 NAICS Code                                  1016\n",
            "2017 NAICS Title                                 1016\n",
            "GHG                                              1016\n",
            "Unit                                             1016\n",
            "Supply Chain Emission Factors without Margins    1016\n",
            "Margins of Supply Chain Emission Factors         1016\n",
            "Supply Chain Emission Factors with Margins       1016\n",
            "Reference USEEIO Code                            1016\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "Activity Count: Business Activity Description    1374\n",
            "Vendor                           1374\n",
            "Cost_USD                         1374\n",
            "Comment                          1374\n",
            "2017 NAICS Title                 1374\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch"
      ],
      "metadata": {
        "id": "dAHaixffDjL2"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "FDj8nmBtH_WD"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_dict = {value: idx for idx, value in enumerate(activity_df['2017 NAICS Title'].unique())}\n",
        "activity_df['label'] = activity_df['2017 NAICS Title'].map(label_dict)\n"
      ],
      "metadata": {
        "id": "lImmsdUSgblo"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Helper functions\n",
        "def introduce_minor_errors(text):\n",
        "    \"\"\"Introduce minor spelling mistakes in the text.\"\"\"\n",
        "    errors_introduced = 0\n",
        "    max_errors = random.randint(2, 3)  # Decide to introduce 2 or 3 minor errors\n",
        "\n",
        "    while errors_introduced < max_errors and len(text) > 4:  # Ensure text is long enough to alter\n",
        "        error_type = random.choice(['substitute', 'omit', 'swap'])\n",
        "        error_index = random.randint(1, len(text) - 2)  # Avoid beginning and end of the text for simplicity\n",
        "\n",
        "        if error_type == 'substitute':\n",
        "            # Substitute a character with a nearby character (mimicking common typing errors)\n",
        "            substitutions = {'a': 's', 's': 'a', 'd': 'f', 'i': 'o', 'o': 'p', 'e': 'r', 'r': 't'}\n",
        "            if text[error_index] in substitutions:\n",
        "                text = text[:error_index] + substitutions[text[error_index]] + text[error_index + 1:]\n",
        "                errors_introduced += 1\n",
        "\n",
        "        elif error_type == 'omit':\n",
        "            # Omit a character\n",
        "            text = text[:error_index] + text[error_index + 1:]\n",
        "            errors_introduced += 1\n",
        "\n",
        "        elif error_type == 'swap':\n",
        "            # Swap two adjacent characters\n",
        "            if error_index < len(text) - 1:  # Ensure there's a character to swap with\n",
        "                text = text[:error_index] + text[error_index + 1] + text[error_index] + text[error_index + 2:]\n",
        "                errors_introduced += 1\n",
        "    return text\n",
        "\n",
        "def introduce_major_errors(text):\n",
        "    \"\"\"Replace or scramble parts of the text to introduce major errors.\"\"\"\n",
        "    # Randomly choose between scrambling or inserting irrelevant text\n",
        "    if random.random() < 0.5:\n",
        "        return ''.join(random.sample(text, len(text)))\n",
        "    else:\n",
        "        return \"Irrelevant text \" + ''.join(random.sample(text, len(text)))\n",
        "    return text\n",
        "\n",
        "# Function to randomly apply either minor or major errors to a text\n",
        "def apply_random_error(text):\n",
        "    if random.random() < 0.40:  # 15% chance to introduce an error\n",
        "        #error_type = random.choice(['minor', 'major'])\n",
        "        #if error_type == 'minor':\n",
        "        #    return introduce_minor_errors(text)\n",
        "        #else:\n",
        "            return introduce_major_errors(text)   # only major errors\n",
        "    return text\n",
        "\n",
        "\n",
        "def apply_errors_with_limit(row, fields, max_errors=2):\n",
        "    \"\"\"\n",
        "    Randomly apply errors to a limited number of fields in a row.\n",
        "\n",
        "    Parameters:\n",
        "    - row: The DataFrame row to apply errors to.\n",
        "    - fields: A list of field names to potentially apply errors to.\n",
        "    - max_errors: Maximum number of fields to apply errors to.\n",
        "    \"\"\"\n",
        "    # Randomly decide how many fields to apply errors to (0 to max_errors)\n",
        "    errors_to_apply = random.randint(0, max_errors)\n",
        "\n",
        "    # Randomly select the fields where errors will be applied\n",
        "    fields_with_errors = random.sample(fields, errors_to_apply)\n",
        "\n",
        "    # Apply errors to the selected fields\n",
        "    for field in fields:\n",
        "        if field in fields_with_errors:\n",
        "            row[field + ' Error'] = apply_random_error(row[field])\n",
        "        else:\n",
        "            row[field + ' Error'] = row[field]\n",
        "\n",
        "    return row"
      ],
      "metadata": {
        "id": "Me4NOK20ggoX"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Define the fields to potentially introduce errors\n",
        "fields = ['Business Activity Description', 'Vendor', 'Comment']\n",
        "\n",
        "# Apply errors to 2 or fewer fields for each row\n",
        "activity_df = activity_df.apply(lambda row: apply_errors_with_limit(row, fields), axis=1)\n",
        "\n",
        "# Combine the possibly altered text fields into a new 'combined_text' column\n",
        "activity_df['combined_text'] = activity_df['Business Activity Description Error'] + \" \" + activity_df['Vendor Error'] + \" \" + activity_df['Comment Error']\n",
        "# Now, 'combined_text' contains the concatenated texts with either minor or major errors introduced\n",
        "\n",
        "\n",
        "# Split data into features and labels\n",
        "# activity_df['combined_text'] = activity_df['Business Activity Description'] + \" \" + activity_df['Vendor'] + \" \" + activity_df['Comment']\n",
        "X = activity_df['combined_text']  # Feature\n",
        "y = activity_df['label']  # Assuming 'label' is already encoded as numeric labels\n",
        "\n",
        "# Splitting dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and fit the TF-IDF vectorizer on training data\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # You can adjust max_features as needed\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Initialize and fit the RandomForestClassifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust parameters as needed\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Transform the test data using the same TF-IDF vectorizer\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = clf.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# You can now use clf to make predictions on new data using the same tfidf_vectorizer to transform the new data\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMQKWjTvxZuX",
        "outputId": "22268969-41ff-4372-8f7d-aa77f43e83c1"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        25\n",
            "           1       0.93      0.96      0.94        26\n",
            "           2       0.97      0.91      0.94        32\n",
            "           3       0.90      0.90      0.90        20\n",
            "           4       0.83      0.93      0.88        27\n",
            "           5       1.00      0.94      0.97        18\n",
            "           6       1.00      1.00      1.00        24\n",
            "           7       1.00      0.89      0.94        27\n",
            "           8       1.00      1.00      1.00        24\n",
            "           9       0.97      1.00      0.98        29\n",
            "          10       0.96      1.00      0.98        23\n",
            "\n",
            "    accuracy                           0.96       275\n",
            "   macro avg       0.96      0.96      0.96       275\n",
            "weighted avg       0.96      0.96      0.96       275\n",
            "\n",
            "Accuracy: 0.9563636363636364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Assuming you've already fitted this with your training data\n",
        "\n",
        "# Assuming clf is your trained RandomForestClassifier and tfidf_vectorizer is your fitted TF-IDF vectorizer\n",
        "\n",
        "test_df['encoded_labels'] = test_df['2017 NAICS Title'].apply(lambda x: label_dict.get(x, -1))  # Unseen labels get -1\n",
        "\n",
        "# Apply errors to 2 or fewer fields for each row\n",
        "test_df = test_df.apply(lambda row: apply_errors_with_limit(row, fields), axis=1)\n",
        "# Combine the possibly altered text fields into a new 'combined_text' column\n",
        "test_df['combined_text'] = test_df['Business Activity Description Error'] + \" \" + test_df['Vendor Error'] + \" \" + test_df['Comment Error']\n",
        "\n",
        "# Prepare the test data features and labels\n",
        "# test_df['combined_text'] = test_df['Business Activity Description'] + \" \" + test_df['Vendor'] + \" \" + test_df['Comment']\n",
        "\n",
        "X_test_new = test_df['combined_text']  # Feature\n",
        "y_test_new = test_df['encoded_labels']  # Replace 'label' with the actual column name for labels in your test data\n",
        "\n",
        "# Transform the test data using the already fitted TF-IDF vectorizer\n",
        "X_test_new_tfidf = tfidf_vectorizer.transform(X_test_new)\n",
        "\n",
        "# Make predictions on the new test data\n",
        "y_pred_new = clf.predict(X_test_new_tfidf)\n",
        "\n",
        "# Evaluate the model on the new test data\n",
        "print(\"New Test Data - Classification Report:\\n\", classification_report(y_test_new, y_pred_new))\n",
        "print(\"New Test Data - Accuracy:\", accuracy_score(y_test_new, y_pred_new))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo4m24ijkcmP",
        "outputId": "d87eed35-e0f0-4c44-bf7d-0788914f38d6"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Test Data - Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      1.00      0.96        24\n",
            "           1       0.94      0.89      0.92        19\n",
            "           2       1.00      1.00      1.00        20\n",
            "           3       0.92      0.92      0.92        26\n",
            "           4       0.93      1.00      0.96        26\n",
            "           5       0.96      1.00      0.98        23\n",
            "           6       0.96      0.93      0.95        28\n",
            "           7       1.00      0.95      0.97        20\n",
            "           8       0.96      0.89      0.93        28\n",
            "           9       0.94      0.97      0.96        34\n",
            "          10       1.00      0.96      0.98        28\n",
            "\n",
            "    accuracy                           0.96       276\n",
            "   macro avg       0.96      0.96      0.96       276\n",
            "weighted avg       0.96      0.96      0.96       276\n",
            "\n",
            "New Test Data - Accuracy: 0.9565217391304348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert y_pred_new and y_test_new to a DataFrame for easier manipulation\n",
        "results_df = pd.DataFrame({'combined_text': X_test_new, 'Actual Label': y_test_new, 'Predicted Label': y_pred_new})\n",
        "\n",
        "# Filter the DataFrame to only include rows where the prediction failed\n",
        "failed_predictions = results_df[results_df['Actual Label'] != results_df['Predicted Label']]\n",
        "\n",
        "# Display the details of rows with failed predictions\n",
        "print(\"Rows with Failed Predictions:\")\n",
        "pd.set_option('display.max_colwidth', None)  # For pandas versions < 1.0, use -1 instead of None\n",
        "print(failed_predictions[['combined_text']])\n",
        "pd.reset_option('display.max_colwidth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gw_kCzUw5FD",
        "outputId": "7409f4ea-0efd-43eb-e95c-f5c44a1d399c"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows with Failed Predictions:\n",
            "                                                                                                                                      combined_text\n",
            "10                                Irrelevant text n ostSfwenroeer yginki arafrim t MinerGuard Education Services Advanced risk management training.\n",
            "12                                Irrelevant text vilEosntnmmrnsivoe ieg nocrtairne GreenScan Analytics Utilizing advanced monitoring technologies.\n",
            "47                Irrelevant text talnnau gt Ptusilinrdesimfxblia on FlowTech Plumbing Irrelevant text stae.is  ltqegHro-iieyhrxuc gerauud ttaw ufe\n",
            "108                                            mn sgsadea  ailanpsltegodnniaTmi TailingsTech Co. veti. lgtnoiovntI yianlunaianfietrei atgmmssasnb a\n",
            "113                       Irrelevant text oTnrl sspnetrgiircnmdosf eaaroi  d to MineMovers Transport  a fnflotc unofip tiesusrsnrr rmao.roioEettinu\n",
            "115                 Irrelevant text on rnrdooeasia latitcsc  nrmnyMiotteesmeae MineHeal Environmental Restoring mined lands to their natural state.\n",
            "133                                 Irrelevant text Mr nicrnecdsegvloislt syoo tCuaReidifcErrnoa td Includes gear selection and maintenance advice.\n",
            "171                              etetaniueoaln yiKddhhc  RenalEd Partners Collaborations with healthcare professionals for comprehensive education.\n",
            "188         Fnodaeagtlihto yenueml ngcm Liquidus Management Devices Irrelevant text  ssni lrgtaet-stmeitetosseebecpucmanCeztpea o .nidneti m fettit\n",
            "221                        Irrelevant text udtl we snanlBle tiaireosa nnmgepq BowlEssentials Store  lnmeesdqixa ieooenrrinet uPiv.e ptopfvedcig crt\n",
            "248             dorVcvgui eaaaioittinsesnn  dtterr odre VoteReady Campaigns Irrelevant text inb reoeuog on rtitpsthacn. vghnecnminnpeawolf aisac E \n",
            "261  Irrelevant text nng dmearsiordoed ntimunF anaagn PledgeProspect Initiative  eilifoeng agfamrs legsendctniece itpiupsoffvr mrtaneiet.ialsuIct s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that '2017 NAICS Title' in both dataframes are of the same format for accurate mapping\n",
        "# Map 'Supply Chain Emission Factors without Margins' from factor_df to test_df based on '2017 NAICS Title'\n",
        "test_df['Emission Factor'] = test_df['2017 NAICS Title'].map(\n",
        "    factor_df.set_index('2017 NAICS Title')['Supply Chain Emission Factors without Margins']\n",
        ")\n",
        "\n",
        "# Calculate the emissions for each row before grouping\n",
        "test_df['Calculated_Emissions'] = test_df['Emission Factor'] * test_df['Cost_USD']\n",
        "\n",
        "# Selecting unique '2017 NAICS Title' and their corresponding 'Emission Factor' to avoid duplicates\n",
        "unique_emission_factors = test_df[['2017 NAICS Title', 'Emission Factor']].drop_duplicates()\n",
        "\n",
        "# Step 1: Match NAICS Titles in test_df with those in factor_df\n",
        "# This step is simplified due to direct matching by '2017 NAICS Title'.\n",
        "# In real-world scenarios, consider complexities of matching titles.\n",
        "\n",
        "# Group 'test_df' by '2017 NAICS Title' and sum the 'Calculated_Emissions' for each group, also count the occurrences\n",
        "aggregated_emissions = test_df.groupby('2017 NAICS Title').agg(\n",
        "    Total_Emissions=('Calculated_Emissions', 'sum'),\n",
        "    Count=('Calculated_Emissions', 'count')\n",
        ").reset_index()\n",
        "\n",
        "# Print the total emissions and count for each title\n",
        "for index, row in aggregated_emissions.iterrows():\n",
        "    print(f\"Title: {row['2017 NAICS Title']}, Total Emissions: {row['Total_Emissions']:.2f}, Count: {row['Count']}\")\n",
        "\n",
        "\n",
        "# Calculate and print overall totals using DataFrame functions\n",
        "total_count = aggregated_emissions['Count'].sum()\n",
        "total_emissions = aggregated_emissions['Total_Emissions'].sum()\n",
        "\n",
        "print(f\"\\nTotal Count of All Rows: {total_count}\")\n",
        "print(f\"Total of Total Emissions: {total_emissions:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWxLDqHSwg64",
        "outputId": "af59244a-945c-4fce-b4f4-9de9f652c2ff"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Automobile Driving Schools, Total Emissions: 1457.40, Count: 28\n",
            "Title: Bowling Centers, Total Emissions: 5716.62, Count: 34\n",
            "Title: Historical Sites, Total Emissions: 2015.28, Count: 23\n",
            "Title: Kidney Dialysis Centers, Total Emissions: 1455.50, Count: 26\n",
            "Title: New Single-Family Housing Construction (except For-Sale Builders), Total Emissions: 1744.59, Count: 20\n",
            "Title: Offices of Dentists, Total Emissions: 886.12, Count: 20\n",
            "Title: Political Organizations, Total Emissions: 1689.65, Count: 28\n",
            "Title: Sewage Treatment Facilities, Total Emissions: 8468.04, Count: 26\n",
            "Title: Support Activities for Metal Mining, Total Emissions: 5052.92, Count: 19\n",
            "Title: Timber Tract Operations, Total Emissions: 3185.31, Count: 24\n",
            "Title: Uranium-Radium-Vanadium Ore Mining, Total Emissions: 15093.87, Count: 28\n",
            "\n",
            "Total Count of All Rows: 276\n",
            "Total of Total Emissions: 46765.30\n"
          ]
        }
      ]
    }
  ]
}
