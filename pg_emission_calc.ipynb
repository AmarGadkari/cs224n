{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQKBtE7_AUwP",
        "outputId": "5a3c3c42-dfbd-4d26-816c-4b5a2dac1e2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-2.5.1-py3-none-any.whl (156 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.38.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: accelerate, sentence-transformers\n",
            "Successfully installed accelerate-0.27.2 sentence-transformers-2.5.1\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.38.1)\n",
            "Collecting transformers[torch]\n",
            "  Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.27.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.38.1\n",
            "    Uninstalling transformers-4.38.1:\n",
            "      Successfully uninstalled transformers-4.38.1\n",
            "Successfully installed transformers-4.38.2\n",
            "Accelerate version: 0.27.2\n",
            "Transformers version: 4.38.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas==1.5.3\n",
        "!pip install sentence-transformers accelerate -U\n",
        "!pip install transformers[torch] -U\n",
        "# !pip install accelerate -U --force-reinstall\n",
        "\n",
        "import accelerate\n",
        "import transformers\n",
        "print(\"Accelerate version:\", accelerate.__version__)\n",
        "print(\"Transformers version:\", transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive2.auth import GoogleAuth\n",
        "from pydrive2.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "kisSICzTaibq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_folder = '/content/drive/My Drive/224_project/'\n",
        "# Specify Google Drive folder ID and file names\n",
        "folder_id = \"1FwqVD1UvjaoSC8UksUBhVkPzPQy95iTg\"\n",
        "train_file_name = 'sustainability_business_activities_training.csv'\n",
        "test_file_name = 'sustainability_business_activities_test.csv'\n",
        "EPA_file_name = 'EPA_EmissionData.csv'\n",
        "\n",
        "# Define the fields to potentially introduce errors\n",
        "fields = ['Business Activity Description', 'Business Activity Vendor', 'Business Activity Comment']\n",
        "num_max_error_columns = 1\n",
        "naics_title_column_name = '2017 NAICS Title'\n",
        "activity_cost_column_name = 'Business Activity Cost USD'\n",
        "epa_emission_factor_column_name = 'Supply Chain Emission Factors without Margins'\n",
        "separator_string = \" [SEP] \"\n",
        "error_column_append_text = \" Error\"\n",
        "combined_text_column_name = 'combined_text'\n",
        "encoded_label_column_name = 'encoded_labels'\n",
        "\n",
        "demo_mode = True\n",
        "\n",
        "def find_file_id_by_name(drive, folder_id, file_name):\n",
        "    \"\"\"Search for a file by name in the specified Google Drive folder.\"\"\"\n",
        "    query = f\"'{folder_id}' in parents and trashed=false and title='{file_name}'\"\n",
        "    file_list = drive.ListFile({'q': query}).GetList()\n",
        "    return file_list[0]['id'] if file_list else None\n",
        "\n",
        "epa_file_id = find_file_id_by_name(drive, folder_id, EPA_file_name)\n",
        "epa_data = drive.CreateFile({'id': epa_file_id})\n",
        "epa_data.GetContentFile(EPA_file_name)\n",
        "factor_df = pd.read_csv(EPA_file_name)\n",
        "print(f\"{len(factor_df)} of rows in factor csv\")\n",
        "# print(factor_df.head())\n",
        "\n",
        "train_file_id = find_file_id_by_name(drive, folder_id, train_file_name)\n",
        "activity_data = drive.CreateFile({'id': train_file_id})\n",
        "activity_data.GetContentFile(train_file_name)\n",
        "activity_df = pd.read_csv(train_file_name)\n",
        "print(f\"{len(activity_df)} of rows in training csv\")\n",
        "# print(activity_df.head())\n",
        "\n",
        "# Load the test data\n",
        "test_file_id = find_file_id_by_name(drive, folder_id, test_file_name)\n",
        "activity_test_data = drive.CreateFile({'id': test_file_id})\n",
        "activity_test_data.GetContentFile(test_file_name)\n",
        "test_df = pd.read_csv(test_file_name)\n",
        "print(f\"{len(test_df)} of rows in test csv\")\n",
        "# print(test_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gK8yCqGkakrk",
        "outputId": "6d765909-0284-4e65-bcc1-4a96f0c30d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1016 of rows in factor csv\n",
            "13744 of rows in training csv\n",
            "2756 of rows in test csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch"
      ],
      "metadata": {
        "id": "dAHaixffDjL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "FDj8nmBtH_WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_dict = {value: idx for idx, value in enumerate(activity_df[naics_title_column_name].unique())}\n",
        "activity_df[encoded_label_column_name] = activity_df[naics_title_column_name].map(label_dict)\n"
      ],
      "metadata": {
        "id": "lImmsdUSgblo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Helper functions\n",
        "def introduce_minor_errors(text):\n",
        "    \"\"\"Introduce minor spelling mistakes in the text.\"\"\"\n",
        "    errors_introduced = 0\n",
        "    max_errors = random.randint(2, 3)  # Decide to introduce 2 or 3 minor errors\n",
        "\n",
        "    while errors_introduced < max_errors and len(text) > 4:  # Ensure text is long enough to alter\n",
        "        error_type = random.choice(['substitute', 'omit', 'swap'])\n",
        "        error_index = random.randint(1, len(text) - 2)  # Avoid beginning and end of the text for simplicity\n",
        "\n",
        "        if error_type == 'substitute':\n",
        "            # Substitute a character with a nearby character (mimicking common typing errors)\n",
        "            substitutions = {'a': 's', 's': 'a', 'd': 'f', 'i': 'o', 'o': 'p', 'e': 'r', 'r': 't'}\n",
        "            if text[error_index] in substitutions:\n",
        "                text = text[:error_index] + substitutions[text[error_index]] + text[error_index + 1:]\n",
        "                errors_introduced += 1\n",
        "\n",
        "        elif error_type == 'omit':\n",
        "            # Omit a character\n",
        "            text = text[:error_index] + text[error_index + 1:]\n",
        "            errors_introduced += 1\n",
        "\n",
        "        elif error_type == 'swap':\n",
        "            # Swap two adjacent characters\n",
        "            if error_index < len(text) - 1:  # Ensure there's a character to swap with\n",
        "                text = text[:error_index] + text[error_index + 1] + text[error_index] + text[error_index + 2:]\n",
        "                errors_introduced += 1\n",
        "    return text\n",
        "\n",
        "def introduce_major_errors(text):\n",
        "    \"\"\"Replace or scramble parts of the text to introduce major errors.\"\"\"\n",
        "    # Randomly choose between scrambling or inserting irrelevant text\n",
        "    if random.random() < 0.5:\n",
        "        return ''.join(random.sample(text, len(text)))\n",
        "    else:\n",
        "        return \"Irrelevant text \" + ''.join(random.sample(text, len(text)))\n",
        "    return text\n",
        "\n",
        "# Function to randomly apply either minor or major errors to a text\n",
        "def apply_random_error(text):\n",
        "    if random.random() < 0.20:  # 15% chance to introduce an error\n",
        "        #error_type = random.choice(['minor', 'major'])\n",
        "        #if error_type == 'minor':\n",
        "        #    return introduce_minor_errors(text)\n",
        "        #else:\n",
        "            return introduce_major_errors(text)   # only major errors\n",
        "    return text\n",
        "\n",
        "\n",
        "def apply_errors_with_limit(row, fields, max_errors=num_max_error_columns):\n",
        "    \"\"\"\n",
        "    Randomly apply errors to a limited number of fields in a row.\n",
        "\n",
        "    Parameters:\n",
        "    - row: The DataFrame row to apply errors to.\n",
        "    - fields: A list of field names to potentially apply errors to.\n",
        "    - max_errors: Maximum number of fields to apply errors to.\n",
        "    \"\"\"\n",
        "    # Randomly decide how many fields to apply errors to (0 to max_errors)\n",
        "    errors_to_apply = random.randint(0, max_errors)\n",
        "\n",
        "    # Randomly select the fields where errors will be applied\n",
        "    fields_with_errors = random.sample(fields, errors_to_apply)\n",
        "\n",
        "    # Apply errors to the selected fields\n",
        "    for field in fields:\n",
        "        if field in fields_with_errors:\n",
        "            row[field + error_column_append_text] = apply_random_error(row[field])\n",
        "        else:\n",
        "            row[field + error_column_append_text] = row[field]\n",
        "\n",
        "    return row\n",
        "\n",
        "def combine_text_fields(row, fields):\n",
        "    \"\"\"\n",
        "    Combine multiple text fields into a single combined text string.\n",
        "\n",
        "    Parameters:\n",
        "    - row: A DataFrame row containing the text fields.\n",
        "    - fields: A list of field names to be combined.\n",
        "\n",
        "    Returns:\n",
        "    - combined_text: A string containing the combined text from the specified fields.\n",
        "    \"\"\"\n",
        "    combined_parts = []\n",
        "    for field in fields:\n",
        "        # Assuming the 'Error' versions of fields are already in the DataFrame\n",
        "        error_field_name = f\"{field}{error_column_append_text}\"\n",
        "        if error_field_name in row:\n",
        "            field_label = field.replace(\" \", \"_\")  # Replace spaces with underscores for label\n",
        "            combined_parts.append(f\"{field_label}: {row[error_field_name]}\")\n",
        "    combined_text = separator_string.join(combined_parts)\n",
        "    return combined_text"
      ],
      "metadata": {
        "id": "Me4NOK20ggoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "current_date = datetime.now()\n",
        "# Format month as 3-letter abbreviation and day as a number\n",
        "formatted_date = current_date.strftime('%b%d')\n",
        "\n",
        "# Apply errors to 2 or fewer fields for each row\n",
        "activity_df = activity_df.apply(lambda row: apply_errors_with_limit(row, fields), axis=1)\n",
        "# Apply the function to each row of the DataFrame to create the 'combined_text' column\n",
        "activity_df[combined_text_column_name] = activity_df.apply(lambda row: combine_text_fields(row, fields), axis=1)\n",
        "# Now, 'combined_text' contains the concatenated texts with either minor or major errors introduced\n",
        "temp_train_file_name = f'train_{formatted_date}.csv'\n",
        "activity_df.to_csv(temp_train_file_name)\n",
        "\n",
        "# Apply errors to 2 or fewer fields for each row\n",
        "test_df = test_df.apply(lambda row: apply_errors_with_limit(row, fields), axis=1)\n",
        "# Combine the possibly altered text fields into a new 'combined_text' column\n",
        "test_df[combined_text_column_name] = test_df.apply(lambda row: combine_text_fields(row, fields), axis=1)\n",
        "temp_test_file_name = f'test_{formatted_date}.csv'\n",
        "test_df.to_csv(temp_test_file_name)\n"
      ],
      "metadata": {
        "id": "qEYB9Tfq6dfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Split data into features and labels\n",
        "# activity_df['combined_text'] = activity_df['Business Activity Description'] + \" \" + activity_df['Vendor'] + \" \" + activity_df['Comment']\n",
        "X = activity_df[combined_text_column_name]  # Feature\n",
        "y = activity_df[encoded_label_column_name]  # Assuming 'label' is already encoded as numeric labels\n",
        "\n",
        "# Splitting dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Initialize and fit the TF-IDF vectorizer on training data\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # You can adjust max_features as needed\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n"
      ],
      "metadata": {
        "id": "KMQKWjTvxZuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from scipy.stats import randint as sp_randint\n",
        "\n",
        "# Define the parameter grid\n",
        "param_dist = {\n",
        "    \"n_estimators\": [600, 800, 1000, 1200, 1400],  # Increased range based on previous best\n",
        "    \"max_depth\": [50, 60, 80],  # Explore deeper trees and some constraints\n",
        "    \"min_samples_split\": [8, 10, 12, 15, 18],  # Narrowing down around the best found value\n",
        "    \"min_samples_leaf\": [1, 2, 3],  # Exploring around the best found value\n",
        "    \"max_features\": ['sqrt', 'log2', None]  # Adding exploration around 'log2'\n",
        "}\n",
        "\n",
        "if demo_mode:\n",
        "  print(\"In the demo mode, not running the random_search....\")\n",
        "else:\n",
        "  # Initialize the RandomForest model\n",
        "  clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "  # Initialize RandomizedSearchCV\n",
        "  n_iter_search = 20\n",
        "  random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search, cv=5, scoring='accuracy')\n",
        "\n",
        "  # Fit the model\n",
        "  random_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "  print(\"Best parameters:\", random_search.best_params_)\n",
        "  # Best parameters: {'n_estimators': 800, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 60}\n",
        "  # Best parameters: {'n_estimators': 1200, 'min_samples_split': 15, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 60}\n",
        "  # Best parameters: {'n_estimators': 1000, 'min_samples_split': 15, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 80}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eZn7ehklZdh",
        "outputId": "89ced511-d993-4a35-d42b-20ac8bd4237d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the demo mode, not running the random_search....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Previously selected best parameter sets\n",
        "selected_params = [\n",
        "    {'n_estimators': 800, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 60},\n",
        "    {'n_estimators': 1200, 'min_samples_split': 15, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 60},\n",
        "    {'n_estimators': 1000, 'min_samples_split': 15, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 80}\n",
        "]\n",
        "\n",
        "# Choose a parameter set (e.g., the first one)\n",
        "chosen_params = selected_params[0]\n",
        "if demo_mode:\n",
        "  print(\"In demo mode, moving forward with params: {chosen_params}\")\n",
        "else:\n",
        "  chosen_params = random_search.best_params_\n",
        "# Initialize the RandomForest model with the chosen parameters\n",
        "best_clf = RandomForestClassifier(**chosen_params, random_state=42)\n",
        "# Fit the model\n",
        "best_clf.fit(X_train_tfidf, y_train)\n",
        "# Transform the test data using the same TF-IDF vectorizer\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "# Make predictions on the test data\n",
        "y_pred = best_clf.predict(X_test_tfidf)\n",
        "\n",
        "# Convert y_test to a DataFrame for easier manipulation\n",
        "y_test_df = pd.DataFrame(y_test)\n",
        "y_test_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Convert y_pred to a DataFrame\n",
        "y_pred_df = pd.DataFrame(y_pred, columns=['Predicted'])\n",
        "\n",
        "# Concatenate the true labels, predicted labels, and the original text features for comparison\n",
        "results_df = pd.concat([y_test_df, y_pred_df, X_test.reset_index(drop=True)], axis=1)\n",
        "results_df.columns = ['True_Label', 'Predicted_Label', 'Features']\n",
        "\n",
        "# Filter to find where predictions were incorrect\n",
        "incorrect_predictions = results_df[results_df['True_Label'] != results_df['Predicted_Label']]\n",
        "\n",
        "'''\n",
        "# Add a prefix to 'Features' to indicate these are from true labels\n",
        "incorrect_predictions['Features_True_Label'] = 'True label: ' + incorrect_predictions['Features']\n",
        "# Since 'Features' based on 'Predicted_Label' doesn't exist directly, we replicate 'Features' column\n",
        "# and add a prefix to indicate these are hypothetically what you'd expect for predicted labels\n",
        "# Note: This does not fetch different 'Features' text for 'Predicted_Label'; it's illustrative\n",
        "incorrect_predictions['Features_Predicted_Label'] = 'Predicted label: ' + incorrect_predictions['Features']\n",
        "'''\n",
        "\n",
        "# Set option to display full text without truncation for investigation\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "print(f\"Rows {len(incorrect_predictions)} in Incorrect Predictions: \")\n",
        "print(incorrect_predictions[['True_Label', 'Predicted_Label', 'Features']])\n",
        "# Reset display option\n",
        "pd.reset_option('display.max_colwidth')\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLRsANpsoaPY",
        "outputId": "6d4fadf4-e35c-4633-81bf-80d66719d79b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In demo mode, moving forward with params: {chosen_params}\n",
            "Rows 8 in Incorrect Predictions: \n",
            "      True_Label  Predicted_Label  \\\n",
            "550            5                7   \n",
            "698            3                7   \n",
            "1135           3                7   \n",
            "1157           3                7   \n",
            "1168           6                7   \n",
            "1172           8                7   \n",
            "1207           9                0   \n",
            "1927           3                7   \n",
            "\n",
            "                                                                                                                                                                                                                              Features  \n",
            "550                       Business_Activity_Description: Irrelevant text wrl Rea sfgong reoeienayeenwtrenge bmae [SEP] Business_Activity_Vendor: RenewFlow Tech [SEP] Business_Activity_Comment: Contributing to sustainability goals.  \n",
            "698             Business_Activity_Description: tdnm optTncn lsoiedgrssor i foriea ra [SEP] Business_Activity_Vendor: SafeCargo Solutions [SEP] Business_Activity_Comment: Enhanced safety measures for radioactive material transport.  \n",
            "1135   Business_Activity_Description: Irrelevant text ednce iitroiMnatlayrroon som aetsesmact ne [SEP] Business_Activity_Vendor: MineHeal Environmental [SEP] Business_Activity_Comment: Restoring mined lands to their natural state.  \n",
            "1157             Business_Activity_Description: Irrelevant text   isp rtossrdindo encgtfo elraomnTira [SEP] Business_Activity_Vendor: RadVan Logistics [SEP] Business_Activity_Comment: Implementing eco-friendly logistics practices.  \n",
            "1168                          Business_Activity_Description: pqtowee ngri easmdlnn uaseitlna Bl [SEP] Business_Activity_Vendor: BowlEssentials Store [SEP] Business_Activity_Comment: Providing expert advice for equipment selection.  \n",
            "1172                   Business_Activity_Description: masleiydetemHittnosr a [SEP] Business_Activity_Vendor: HemoCare Services [SEP] Business_Activity_Comment: State-of-the-art hemodialysis equipment for efficient blood filtering.  \n",
            "1207  Business_Activity_Description: Irrelevant text inipoeHittnitatmg oa m sckroenidso arrl [SEP] Business_Activity_Vendor: LegacyLure Campaigns [SEP] Business_Activity_Comment: Utilizing social media to reach a broader audience.  \n",
            "1927                        Business_Activity_Description: Woi gietoettamprri ea  nrtnntainnsem [SEP] Business_Activity_Vendor: MineWaterTech [SEP] Business_Activity_Comment: Ensuring water quality exceeds environmental standards.  \n",
            "Accuracy: 0.9970898508548564\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       246\n",
            "           1       1.00      1.00      1.00       250\n",
            "           2       1.00      1.00      1.00       236\n",
            "           3       1.00      0.98      0.99       254\n",
            "           4       1.00      1.00      1.00       264\n",
            "           5       1.00      1.00      1.00       249\n",
            "           6       1.00      1.00      1.00       248\n",
            "           7       0.97      1.00      0.99       246\n",
            "           8       1.00      1.00      1.00       267\n",
            "           9       1.00      1.00      1.00       244\n",
            "          10       1.00      1.00      1.00       245\n",
            "\n",
            "    accuracy                           1.00      2749\n",
            "   macro avg       1.00      1.00      1.00      2749\n",
            "weighted avg       1.00      1.00      1.00      2749\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Assuming you've already fitted this with your training data\n",
        "\n",
        "# Assuming clf is your trained RandomForestClassifier and tfidf_vectorizer is your fitted TF-IDF vectorizer\n",
        "\n",
        "test_df[encoded_label_column_name] = test_df[naics_title_column_name].apply(lambda x: label_dict.get(x, -1))  # Unseen labels get -1\n",
        "\n",
        "X_test_new = test_df[combined_text_column_name]  # Feature\n",
        "y_test_new = test_df[encoded_label_column_name]  # Replace 'label' with the actual column name for labels in your test data\n",
        "\n",
        "# Transform the test data using the already fitted TF-IDF vectorizer\n",
        "X_test_new_tfidf = tfidf_vectorizer.transform(X_test_new)\n",
        "\n",
        "# Make predictions on the new test data\n",
        "y_pred_new = best_clf.predict(X_test_new_tfidf)\n",
        "\n",
        "# Evaluate the model on the new test data\n",
        "print(\"New Test Data - Classification Report:\\n\", classification_report(y_test_new, y_pred_new))\n",
        "print(\"New Test Data - Accuracy:\", accuracy_score(y_test_new, y_pred_new))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo4m24ijkcmP",
        "outputId": "580f25ff-f20f-4459-ac00-2dc2d2da1cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Test Data - Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       257\n",
            "           1       1.00      1.00      1.00       230\n",
            "           2       1.00      1.00      1.00       237\n",
            "           3       1.00      0.99      1.00       252\n",
            "           4       1.00      1.00      1.00       250\n",
            "           5       1.00      1.00      1.00       254\n",
            "           6       1.00      0.99      1.00       242\n",
            "           7       0.98      1.00      0.99       253\n",
            "           8       1.00      1.00      1.00       257\n",
            "           9       1.00      1.00      1.00       253\n",
            "          10       1.00      1.00      1.00       271\n",
            "\n",
            "    accuracy                           1.00      2756\n",
            "   macro avg       1.00      1.00      1.00      2756\n",
            "weighted avg       1.00      1.00      1.00      2756\n",
            "\n",
            "New Test Data - Accuracy: 0.997822931785196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert y_pred_new and y_test_new to a DataFrame for easier manipulation\n",
        "results_df = pd.DataFrame({combined_text_column_name: X_test_new, 'Actual Label': y_test_new, 'Predicted Label': y_pred_new})\n",
        "\n",
        "# Filter the DataFrame to only include rows where the prediction failed\n",
        "failed_predictions = results_df[results_df['Actual Label'] != results_df['Predicted Label']]\n",
        "\n",
        "# Display the details of rows with failed predictions\n",
        "print(\"Rows with Failed Predictions:\")\n",
        "pd.set_option('display.max_colwidth', None)  # For pandas versions < 1.0, use -1 instead of None\n",
        "print(failed_predictions)\n",
        "pd.reset_option('display.max_colwidth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gw_kCzUw5FD",
        "outputId": "4b80eee9-a782-42c7-c3de-ca06d1d3394a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows with Failed Predictions:\n",
            "                                                                                                                                                                                                                        combined_text  \\\n",
            "240                              Business_Activity_Description: Baola ienesllmn tsu ndnt qgeeairwp [SEP] Business_Activity_Vendor: AlleyEquip Sales [SEP] Business_Activity_Comment: Providing expert advice for equipment selection.   \n",
            "1033                Business_Activity_Description: gnnntststEnilialidxida  ahe iob [SEP] Business_Activity_Vendor: ExhibitPast Creators [SEP] Business_Activity_Comment: Incorporating multimedia elements for dynamic presentations.   \n",
            "1070            Business_Activity_Description: hn aneoenallBel dwegica ncg oltnytmonyai [SEP] Business_Activity_Vendor: FrameFix Services [SEP] Business_Activity_Comment: Ensuring optimal lane conditions with regular maintenance.   \n",
            "1216                   Business_Activity_Description: eFrariv snuetosn odnpltim eerernoc a [SEP] Business_Activity_Vendor: FireBreak Solutions [SEP] Business_Activity_Comment: Deploying early detection systems for rapid response.   \n",
            "1238                  Business_Activity_Description: sartio napnnmitngoW iimrnera tete et [SEP] Business_Activity_Vendor: EcoWater Treatment [SEP] Business_Activity_Comment: Ensuring water quality exceeds environmental standards.   \n",
            "2607  Business_Activity_Description: Irrelevant text Wrao tmiiitnen o nntgnrripmatase eet [SEP] Business_Activity_Vendor: EcoWater Treatment [SEP] Business_Activity_Comment: Ensuring water quality exceeds environmental standards.   \n",
            "\n",
            "      Actual Label  Predicted Label  \n",
            "240              6                7  \n",
            "1033             9                7  \n",
            "1070             6                7  \n",
            "1216             2                7  \n",
            "1238             3                7  \n",
            "2607             3                5  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that '2017 NAICS Title' in both dataframes are of the same format for accurate mapping\n",
        "# Map 'Supply Chain Emission Factors without Margins' from factor_df to test_df based on '2017 NAICS Title'\n",
        "test_df['Emission Factor'] = test_df[naics_title_column_name].map(\n",
        "    factor_df.set_index(naics_title_column_name)[epa_emission_factor_column_name]\n",
        ")\n",
        "\n",
        "# Calculate the emissions for each row before grouping\n",
        "test_df['Calculated_Emissions'] = test_df['Emission Factor'] * test_df[activity_cost_column_name]\n",
        "\n",
        "# Selecting unique '2017 NAICS Title' and their corresponding 'Emission Factor' to avoid duplicates\n",
        "unique_emission_factors = test_df[[naics_title_column_name, 'Emission Factor']].drop_duplicates()\n",
        "\n",
        "# Step 1: Match NAICS Titles in test_df with those in factor_df\n",
        "# This step is simplified due to direct matching by '2017 NAICS Title'.\n",
        "# In real-world scenarios, consider complexities of matching titles.\n",
        "\n",
        "# Group 'test_df' by '2017 NAICS Title' and sum the 'Calculated_Emissions' for each group, also count the occurrences\n",
        "aggregated_emissions = test_df.groupby(naics_title_column_name).agg(\n",
        "    Total_Emissions=('Calculated_Emissions', 'sum'),\n",
        "    Count=('Calculated_Emissions', 'count')\n",
        ").reset_index()\n",
        "\n",
        "# Print the total emissions and count for each title\n",
        "for index, row in aggregated_emissions.iterrows():\n",
        "    print(f\"Title: {row[naics_title_column_name]}, Total Emissions: {row['Total_Emissions']:.2f}, Count: {row['Count']}\")\n",
        "\n",
        "\n",
        "# Calculate and print overall totals using DataFrame functions\n",
        "total_count = aggregated_emissions['Count'].sum()\n",
        "total_emissions = aggregated_emissions['Total_Emissions'].sum()\n",
        "\n",
        "print(f\"\\nTotal Count of All Rows: {total_count}\")\n",
        "print(f\"Total of Total Emissions: {total_emissions:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWxLDqHSwg64",
        "outputId": "e4c016ed-4005-4c77-dd1a-b26dd05e480a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Automobile Driving Schools, Total Emissions: 15331.12, Count: 250\n",
            "Title: Bowling Centers, Total Emissions: 41183.58, Count: 242\n",
            "Title: Historical Sites, Total Emissions: 19081.32, Count: 253\n",
            "Title: Kidney Dialysis Centers, Total Emissions: 14414.15, Count: 257\n",
            "Title: New Single-Family Housing Construction (except For-Sale Builders), Total Emissions: 22194.55, Count: 271\n",
            "Title: Offices of Dentists, Total Emissions: 9963.62, Count: 230\n",
            "Title: Political Organizations, Total Emissions: 16379.77, Count: 257\n",
            "Title: Sewage Treatment Facilities, Total Emissions: 80527.14, Count: 254\n",
            "Title: Support Activities for Metal Mining, Total Emissions: 68293.40, Count: 253\n",
            "Title: Timber Tract Operations, Total Emissions: 27541.68, Count: 237\n",
            "Title: Uranium-Radium-Vanadium Ore Mining, Total Emissions: 137873.47, Count: 252\n",
            "\n",
            "Total Count of All Rows: 2756\n",
            "Total of Total Emissions: 452783.82\n"
          ]
        }
      ]
    }
  ]
}
